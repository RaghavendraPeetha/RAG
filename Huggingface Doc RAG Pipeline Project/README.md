# RAG Pipeline with Milvus, Sentence Transformers, and Qwen

## ğŸ“Œ Project Overview

This project implements an endâ€‘toâ€‘end **Retrievalâ€‘Augmented Generation
(RAG)** pipeline using:

-   Sentence Transformers for embeddings
-   Milvus as a vector database
-   Hugging Face Transformers for LLM generation (Qwen)
-   Python for orchestration

The system retrieves relevant document chunks using semantic search and
generates grounded answers using a large language model.

------------------------------------------------------------------------

## ğŸ—ï¸ Architecture

1.  Document Chunking â†’ Split documents into overlapping chunks\
2.  Embedding Generation â†’ Convert text chunks into vector embeddings\
3.  Vector Storage â†’ Store embeddings in Milvus\
4.  Retrieval â†’ Fetch topâ€‘k relevant chunks\
5.  Generation â†’ Generate answer using retrieved context

------------------------------------------------------------------------

## ğŸ“‚ Project Structure

    .
    â”œâ”€â”€ chunking.py
    â”œâ”€â”€ embeddings.py
    â”œâ”€â”€ milvus_insert.py
    â”œâ”€â”€ retrieval.py
    â”œâ”€â”€ generation.py
    â”œâ”€â”€ main.ipynb / main.py
    â””â”€â”€ README.md

------------------------------------------------------------------------

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone repository

    git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git
    cd YOUR_REPO

### 2ï¸âƒ£ Create virtual environment

    python -m venv venv
    source venv/bin/activate   # Linux/Mac
    venv\Scripts\activate      # Windows

### 3ï¸âƒ£ Install dependencies

    pip install -r requirements.txt

------------------------------------------------------------------------

## ğŸ“¦ Requirements

-   Python 3.9+
-   transformers
-   sentence-transformers
-   pymilvus
-   torch
-   accelerate
-   bitsandbytes (optional)

------------------------------------------------------------------------

## ğŸš€ Usage

### Step 1 --- Prepare documents

Load documents into list of dictionaries:

    documents = [
        {"text": "...", "source": "file1"},
    ]

### Step 2 --- Chunk documents

    chunks = chunk_all_documents(documents)

### Step 3 --- Generate embeddings

    embeddings = generate_embeddings(texts, model)

### Step 4 --- Insert into Milvus

    insert_data_to_milvus(client, collection_name, chunks, embeddings)

### Step 5 --- Retrieve

    docs = retrieve_documents(query, client, collection_name, embedding_model)

### Step 6 --- Generate answer

    result = generate_answer(query, docs, generator)

------------------------------------------------------------------------

## ğŸ§  Prompt Strategy

The model is instructed to answer ONLY from context to reduce
hallucinations.

------------------------------------------------------------------------

## ğŸ“Š Features

âœ… Overlapping chunking\
âœ… Batch embedding generation\
âœ… Vector similarity search\
âœ… Contextâ€‘aware answer generation\
âœ… Modular pipeline\
âœ… Productionâ€‘ready structure

------------------------------------------------------------------------

## ğŸ”® Future Improvements

-   Hybrid search (BM25 + Vector)
-   Crossâ€‘encoder reranking
-   Streaming responses
-   Evaluation metrics
-   API deployment with FastAPI
-   UI with Streamlit

------------------------------------------------------------------------

## ğŸ› ï¸ Tech Stack

-   Python
-   Hugging Face Transformers
-   Sentence Transformers
-   Milvus
-   PyTorch

------------------------------------------------------------------------

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first
to discuss.

------------------------------------------------------------------------

## ğŸ“œ License

MIT License

------------------------------------------------------------------------

## ğŸ‘¤ Author

Peetha Raghavendra\
Machine Learning Engineer

------------------------------------------------------------------------

â­ If you found this useful, please star the repo!
